Is Learning Feasible?
======================

- Example: Χ = { 0, 1 }³, D ⊆ Χ a sample

  ```
         |  Χ_n  |  y_n  |  f_1  f_2  f_3  f_4  f_5  f_7  f_7  f_8
         |-------|-------|------------------------------------------
       { |  000  |  -1   |   0    0    0    0    0    0    0    0
       { |  001  |   1   |   1    1    1    1    1    1    1    1
     D { |  010  |   1   |   1    1    1    1    1    1    1    1
       { |  011  |  -1   |   0    0    0    0    0    0    0    0
       { |  100  |   1   |   1    1    1    1    1    1    1    1
         |-------|-------|------------------------------------------
         |  101  |   ?   |   0    0    0    0    1    1    1    1
         |  110  |   ?   |   0    0    1    1    0    0    1    1
         |  111  |   ?   |   0    1    0    1    0    1    0    1
  ```

  We cannot deduce the results of the boolean function outside of the sample.
  In this case, we list all possible 'f' functions and - as we can see - they
  have a big range of results.

  **1.7**: Η has only two hypothesis: all values outside the sample evaluate to
  0, or all values outside the sample evaluate to 1. The algorithm Α returns a
  hypothesis with the best match for D.

  **a.** How many functions agree with `g` in all 3 points ∈ D? How many agree
  with 2 points? How many agree with 1 point? How many agree with no point?


  - `g` evaluates to zeros
    ```
           |  Χ_n  |  y_n  |  f_1  f_2  f_3  f_4  f_5  f_7  f_7  f_8
           |-------|-------|------------------------------------------
         { |  000  |  -1   |   0    0    0    0    0    0    0    0
         { |  001  |   1   |   1    1    1    1    1    1    1    1
       D { |  010  |   1   |   1    1    1    1    1    1    1    1
         { |  011  |  -1   |   0    0    0    0    0    0    0    0
         { |  100  |   1   |   1    1    1    1    1    1    1    1
           |-------|-------|------------------------------------------
           |  101  |   0   |   0    0    0    0    1    1    1    1
           |  110  |   0   |   0    0    1    1    0    0    1    1
           |  111  |   0   |   0    1    0    1    0    1    0    1
    ```

    1 function agrees with all points (f_1).
    3 functions agree with 2 points (f_2, f_3, f_5).
    3 functions agree with 1 points (f_4, f_6, f_7).
    1 function agrees with no points (f_8).


  - `g` evaluates to ones
    Analogous to the previous example.


- For an unknown P, h ∈ Η fixed, we can infer probabilistically:
  ```
  E_out(h) = IE(Χ~P) [h(x) ≠ f(x)]
  ```
  from:
  ```
  E_in(h) 1/N Σ(i=1,N) [h(x) ≠ f(x)]
  ```

- We can write the Hoeffgen's law as:

  ```
  IP[|E_in(h)-E_out(h)| > ε] ≤ 2e^(-2ε²N)
  ```

  This result does not depend on `E_out(h)`,

- `E_in(h) ≅ E_out(h)` is PAC (probably, aproximately, correct).

- Unhappily, we cannot apply Hoeffgen's law for `g`, as we used as hypothesis
  that the law is valid for a **fixed** `h`.

- We can see the error for `E_out(g)` using the errors for each individual `h`:
  ```
  IP[|E_in(g)-E_out(g)| ≥ ε] ≤ IP[|E_in(h_1)-E_out(h_1)| > ε
                                   or |E_in(h_2)-E_out(h_2)| > ε
                                   ...
                                   or |E_in(h_M)-E_out(h_M)| > ε]

                             ≤ Σ(m=1,M) IP[|E_in(h_m)-E_out(h_m)| > ε]

                             ≤ Σ(m=1,M) 2e^(-2ε²N)

                             ≤ 2Me^(-2ε²N)
  ```

  The problem of this inequality is that: as it depends on N, what happens when
  we have an **infinite** hypothesis set? For this, we'll have the VC-dimension.
