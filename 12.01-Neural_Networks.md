Neural Networks
================

- Forward propagation:
  ```
  x^(0) ← x
  for l=1 to L do
    s^(l) ← (W^(l))x^(l-1)
    s^(l) ← [θ( s^(l) )]
  h(x) ← x^(L)
  ```

- Error `E_in`:
  ```
  E_in = 1/N * Σ(n=1,N) (h(x_n,w)-y_n)²
       = 1/N * Σ(n=1,N) (x_n^(L)-y_n)²
  ```
  Objective: minimize `E_in(w)`

- Descendent gradient: `w(t+1) = w(t) - η∇E_in(w(t))`

- Exercise 7.7: h(x) = tanh(w^Tx)
  ```
  E_in(h) = 1/N * Σ(n=1,N) (tanh(w^Tx_n)-y_n)²
  ```
  Show that:
  ```
  ∇E_in(w) = 2/N * Σ(n=1,N) (tanh(w^Tx)-y_n) * (1-tanh²(w^Tx)) * x_n
  ```

- In order to calculate the descendent gradient, we need the derivatives with
  respect to each weight matrix W^(L):
  ```
  ∂E_in    1    N   ∂l_n
  ------ = - *  Σ  ------ , with l_n = (h_n-y_n)²
  ∂W^(l)   N   n=1 ∂W^(l) 
  ```

  Introducing the sensitivity as: `δ_j^(l) = ∂l/∂s_j^(l)`
  ```
    ∂l        ∂l      ∂s_j^(l)
  ------ = -------- * -------- = x^(l-1) * (δ^(l))^T
  ∂W^(l)   ∂s_j^(l)    ∂W^(l)

  ```
  since we have `s_j^(l) =   Σ(α=1,d^(l-1)) ( w_(d_j)^(l) * x_α^(l-1) )`

- For `l = L` (final layer):
  ```
          ∂l^(w)   ∂ (x^(L)-y)²                   ∂ x^(L)
  δ^(L) = ------ = ------------ = 2 * (x^(L)-y) * -------
          ∂S^(L)      ∂S^(L)                      ∂ S^(L)

        = 2 * (x^(L)-y) * θ'(S^L)
  ```
  We use `x^(L) = θ(S^(L))`.

- Back-propagation (to compute sensitivities):
  ```
  Input: a data point (x,y)

  Run forward propagation on x to compute and save
    s^(l), for l = 1..L
    x^(l), for l = 1..L

  δ^(L) ← 2 * (x^(L)-y) * θ'(s^(L))

  θ'(s^(L)) = 1-(x^(L))²

  x^(l) = [    1     ]
          [ θ(s^(l)) ]

  for l=L-1 to 1 do
    Let θ'(s^(l))  = [1 - x^(l) ⊗ x^(l)]
    Compute the sensitivity δ^(l) from δ^(l+1),
    with δ^(l) = θ'(s^(l)) ⊗ w^(l+1)*δ^(l+1)
  ```
