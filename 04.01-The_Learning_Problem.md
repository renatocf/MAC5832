The Learning Problem
=====================

- Outline
  - Examples of machine learning
  - Components of learning
  - A simple model
  - Types of learning
  - Puzzle


- **Example:** Predicting how a viewer rate a movie
  - 10% improvement = 1 million dollar prize
  - **The essence of machine learning:**
    - A pattern exists
    - We cannot pin it down mathematically
    - We have data on it


- Movie rating: a solution
```
                .-> likes commedy?
                |   .-> likes action?
                |   |   .-> prefers blockbuster?
                |   |   |
                |   |   |                     .-> likes Tom Cruise?
    viewer    | o | O | o |       ---       | o |
                +
                |
                | Match movie     add contributions
                | and viewer      -------------------> predicted rating
                | factors         from each factor
                |
                +
    movie     | O | . | o |       ---       | O |
                |   |   |                     '-> Tom Cruise in it?
                |   |   '-> blockbuster?
                |   '-> action content
                '-> commedy content
```


- The learning approach
```
    viewer    | O | o | O | o | o | ------------------------- | o | . |

    movie     | o | O | . | o | O | ------------------------- | o | O |

                                      ^
                                      |
                                   Rating
```


- Components of learning
  - **Metaphor:** Credit approval
  - Applicant information:

    | age                | 23 years           |
    | gender             | male               |
    | annual salary      | $30,000            |
    | years in residence | 1 year             |
    | years in job       | 1 year             |
    | current debt       | $15,000            |
    |       -----        |       -----        |

    Approve credit?

  - **Formalization**:
    - Input: **x** (_customer application_)
    - Output: y (_good/bad customer?_)
    - Target function: f: Χ → Υ (_ideal credit approval formula_)
    - Data: `(x_1, y_1)`, `(x_2, y_2)`, ..., `(x_N, y_N)` (_historical records_)
    - Hypothesis: g: Χ → Υ (_formula to be used_)

    ```
            .-------------------------.
            | Unknown target function |
            |       f: Χ → Υ          |
            '-------------------------'
              (ideal credit approval)
                        ↓
          .----------------------------.
          |      Training example      |
          | (x_1,y_1), ..., (x_N, y_N) |
          '----------------------------'
      (historial records of credit customers)
                      `.
                        `.    .--------------------.     .------------------.
                          `-> | Learning Algorithm | --> | Final hypothesis |
                          .-> |         Α          |     |      g ≈ f       |
                        .´    '--------------------'     '------------------'
                      .´                                    (final credit
              .----------------.                           approval formula)
              | Hypothesis set |
              |       Η        |
              '----------------'
    ```


- Solution components
  - The 2 solutions of the learning problem:
    - The hypothesis set Η = {h}, g ∈ H
    - The learning algorithm
  - Together, they are referred as the **leraning model**.


- A simple hypothesis set: the **'perceptron'**
  - For input **x** = `(x_1, ..., x_d)`, 'attributes of customer'

  - Assumes 'linearly separable' data:
    ```
        .-------------.     .-------------.
        | -    +      |     |  - \ +      |
        |   -     + + |     |   - \   + + |
        |        +    |     |      \ +    |
        | -  -        |     | -  -  \     |
        |          +  |     |        \ +  |
        '-------------'     '-------------'
    ```

  - Approve credit if `Σ(i=1,d) w_i * x_i > threshold`

  - Deny credit if `Σ(i=1,d) w_i * x_i < threshold`

  - This linear formula h ∈ Η can be written as:
    ```
    h(x) = sign((Σ(i=1,d) w_i * x_i) - threshold)
    ```

  - We can substitute `-threshold = w_0`:
    ```
    h(x) = sign((Σ(i=1,d) w_i * x_i) + w_0)
    ```

  - We can introduce an artificial coordinate `x_0 = 1`:
    ```
    h(x) = sign((Σ(i=0,d) w_i * x_i))
    ```
    This increases the dimension from ℝ^d to ℝ^(d+1).

  - In vector form, the perceptron implements:
    ```
    h(x) = sign(w^Tx)
    ```

- A simple learning algorithm: PLA
  - The perceptron implements:
    ```
    h(x) = sign(w^Tx)
    ```

  - Given the training set:
    ```
    (x_1,y_1), (x_2,y_2), ..., (x_N,y_N)
    ```

  - Pick a **misclassified** point:
    ```
    h(x) = sign(w^Tx_i) ≠ y_i
    ```
    and update the weight vector:
    ```
    w ← w + y_i * x_i
    ```

  - Simbolically, we always get a better vector:
    ```
        .--------.                 .--------.
        | y = +1 |                 | y = -1 |
        '--------'                 '--------'

                   ^                          ^
                .´/ w + yx                 /|/ w
              .´ /                       .´ / 
            .´  /                      .´  /
          .´   /    .> x             .´   /    .> x
        |/    /   .´               .´    /   .´
     w <.    /  .´         w + yx <.    /  .´
         `. / .´                    `. / .´
           `*´                        `*´
    ```

- Iterations of PLA
  - One iteration of PLA:
    ```
    w ← w + y_i * x_i
    ```
    where (**x**,y) is a misclassified training point.

  - At iteration t = 1,2,3,..., pick a misclassified point from:
    ```
    (x_1,y_1), (x_2,y_2), ..., (x_N,y_N)
    ```
    and run PLA iteration from it.

  - That's it!

- An example of Perceptron:

  ```
    ^
    |                     | x_0 | x_1 | x_2 |  y  |
    *      *              |-----|-----|-----|-----| 
    |              **x1** |  1  |  0  |  0  | -1  |
    |              **x2** |  1  |  0  |  1  |  1  |
    +------*-->    **x3** |  1  |  1  |  0  |  1  |
                   **x4** |  1  |  1  |  1  |  1  |
  ```

  From analysing the table, we know that `f(x_1, x_2) = x_1 + x_2`
  (binary or) is the optimal function.

  We'll start with a vector: `w_0 = (1/2, -1, 1)`

  ```
  w_0^T*x1 = ( 1/2 -1 1 )( 1 ) = 1/2 > 0 → Mismatch...
                         ( 0 )
                         ( 0 )

  w_1 = w_0 + x1y1 = ( 1/2 ) + ( 1 ) * -1 = ( 1/2 ) + ( -1 ) = ( -1/2 )
                     (  -1 )   ( 0 )        (  -1 )   (  0 )   (   -1 )
                     (   1 )   ( 0 )        (   1 )   (  0 )   (    1 )

  w_1^T*x2 = ( -1/2 -1 1 )( 1 ) = 1/2 > 0 → Match!
                          ( 0 )
                          ( 1 )

  w_1^T*x3 = ( -1/2 -1 1 )( 1 ) = -3/2 > 0 → Mismatch...
                          ( 1 )
                          ( 0 )

  w_2 = w_1 + x2y2 = ( -1/2 ) + ( 1 ) * 1 = ( -1/2 ) + ( 1 ) = ( 1/2 )
                     (   -1 )   ( 1 )       (   -1 )   ( 1 )   (   0 )
                     (    1 )   ( 0 )       (    1 )   ( 0 )   (   1 )

  w_2^T*x1 = ( 1/2 0 1 )( 1 ) = 3/2 > 0 → Mismatch...
                        ( 0 )
                        ( 0 )

  w_3 = w_2 + x1y1 = ( 1/2 ) + ( 1 ) * -1 = ( 1/2 ) + ( -1 ) = ( -1/2 )
                     (   0 )   ( 0 )        (   0 )   (  0 )   (    0 )
                     (   1 )   ( 0 )        (   1 )   (  0 )   (    1 )

  w_3^T*x2 = ( -1/2 0 1 )( 1 ) = 1/2 > 0 → Match!
                         ( 0 )
                         ( 1 )

  w_3^T*x3 = ( -1/2 0 1 )( 1 ) = -1/2 < 0 → Mismatch...
                         ( 1 )
                         ( 0 )

  w_4 = w_3 + x3y3 = ( -1/2 ) + ( 1 ) * 1 = ( -1/2 ) + ( 1 ) = ( 1/2 )
                     (    0 )   ( 1 )       (    0 )   ( 1 )   (   1 )
                     (    1 )   ( 0 )       (    1 )   ( 0 )   (   1 )

  w_4^T*x1 = ( 1/2 1 1 )( 1 ) = 1/2 > 0 → Mismatch...
                        ( 0 )
                        ( 0 )

  w_5 = w_4 + x1y1 = ( 1/2 ) + ( 1 ) * -1 = ( 1/2 ) + ( -1 ) = ( -1/2 )
                     (   1 )   ( 0 )        (   1 )   (  0 )   (    1 )
                     (   1 )   ( 0 )        (   1 )   (  0 )   (    1 )

  w_1^T*x2 = ( -1/2 1 1 )( 1 ) = 1/2 > 0 → Match!
                         ( 0 )
                         ( 1 )

  w_1^T*x3 = ( -1/2 1 1 )( 1 ) = 1/2 > 0 → Match!
                         ( 1 )
                         ( 0 )

  w_1^T*x4 = ( -1/2 1 1 )( 1 ) = 3/2 > 0 → Match!
                         ( 1 )
                         ( 1 )
  ```
  As the algorithm works for all inputs, we get a line dividing the plan:

  ```
    ^
  `.|                    | x_0 | x_1 | x_2 |  y  |
    *.    *              |-----|-----|-----|-----| 
    | `.          **x1** |  1  |  0  |  0  | -1  |
    |   `.        **x2** |  1  |  0  |  1  |  1  |
    +-----*-->    **x3** |  1  |  1  |  0  |  1  |
            `.    **x4** |  1  |  1  |  1  |  1  |
  ```

