Revisão Álgebra Linear
=======================

- Denota-se por uma maiúscula, digamos `A`, uma matriz A ∈ ℝ^M×N, A\_MxN.

- Denota-se por uma letra minúscula, digamos `x`, um vetor x ∈ ℝ^N.
  Por convenção, `x` é um vetor coluna, com N linhas e 1 coluna.

- Um vetor linha é consegido transpondo o vetor `x`, isto é, `x^T`.

- Em Python, usaremos o **numpy**, que pode ser usado com:
  ```python
  import numpy as np
  x = np.array([0, 4, 9, 12, 15])
  ```

- Um componente de `x`, isto é, `x_i`, é acessado com:
  ```python
  x_i = x[i-1]
  ```
  Caso i >= 1.

  Para a biblioteca do Python, não importa muito se o vetor é linha ou coluna.

- Para criar uma matriz:
  ```python
  A = np.array([[1,2,3], [8,-1,7]])
  ```

- Podemos acessar o tamanho dos métodos da matriz usando o operador `.`:
  ```python
  A.shape # → (2,3)
  ```

- Um elemenento de A, A\_i, é acessado com A[i-1,j-1], i ≥ 1, j ≥ 1.

- Um vetor coluna de A é denotado por `a_j` e um vetor linha, por `a_i`.

- Em Python, "corta-se" a matriz A para obter `a_j` e `a_i`:
  ```python
  ai = A[i-1,:]
  aj = A[:,j-1]
  ```

- **Multiplicação de matrizes**: A ∈ ℝ^M×P, B ∈ ℝ^P×N gera C ∈ ℝ^M×N, C = AB.
  - O problema de multiplicar matrizes em aprendizagem de máquina é que, em
    geral, a ordem das matrizes será muito grande.
  - Em Python, `C = np.dot(A, B)`.
  - Propriedades:
    - Associativa: `(AB)C = A(BC)`
    - Distributiva: `A(B+C) = AB + AC`


- **Produto interno de vetores**: x,y ∈ ℝ^N, chamamos de produto interno a
  operação `x^Ty`.
  ```
               [ y1 ]
    [x1 .. xN ][    ] = Σ(i) x_i × y_i
               [ yN ]
  ```
  - Em Python, `np.inner(x, y)`


- **Produto externo de vetores**: Dados x ∈ ℝ^M e y ∈ ℝ^N, o produto externo de
  `x` por `y^T` pertencerá ao ℝ^M×N:
  ```
           [ x1 ]                   [ x1y1 .. x1yN ]
    xy^T = [ x2 ] [ y1 y2 .. yN ] = [ x2y1 .. x2yN ]
           [ .. ]                   [  ..  ..  ..  ]
           [ xM ]                   [ xMy1 .. nMyN ]
  ```
  - Em Python, `np.outer(x,y)`.


- **Produto de matriz e vetor**: O produto de uma matriz A ∈ ℝ^M×N por um
  vetor x ∈ ℝ^N é um vetor y ∈ ℝ^M. y ∈ ℝ^M, y = Ax. Em Pyhon,
  `y = np.dot(A, x)`.


- **Transposta de uma matriz**: A transposta de uma matriz A ∈ ℝ^M×N
  será A^T ∈ ℝ^N×M, `(A^T)_ij = A_ji`.
  - Em Python, `A^T = A.transpose()` ou `A^T = np.transpose(A)`.
  - Propriedades:
    - `(A^T)^T = A`
    - `(AB)^T = B^TA^T`
    - `(A+B)^T = A^T + B^T`


- **Matrizes simétricas**: Se A ∈ ℝ^N×N é simétrica se A = A^T. Ela é
    anti-simétrica se A = -A^T.
    - Propriedades:
      - ∀ A ∈ ℝ^N×N, vale que `A + A^T` é simétrica.
      - ∀ A ∈ ℝ^N×N, vale que `A - A^T` é anti-simétrica.
      - Então: `A = 1/2 * (A + A^T) + 1/2 (A - A^T)`.


- **Traço de uma matriz**: O traço de uma matriz A ∈ ℝ^N×N é a soma dos
  elementos da diagonal: `Tr(A) = Σ(i) A_ii`.
  - Em Python, `A.trace()` ou `np.trace(A)`.
  - Propriedades:
    - A ∈ ℝ^N×N,   `Tr(A) = Tr(A^T)`
    - A,B ∈ ℝ^N×N, `Tr(A+B) = Tr(A) + Tr(B)`
    - A ∈ ℝ^N×N,   `c ∈ ℝ, Tr(cA) = cTr(A)`
    - A,B ∈ ℝ^N×N, `Tr(A^TB) = Tr(B^TA) = Tr(AB^T) = Tr(BA^T0 = Σ(i,j) Aij*Bij`


- **Posto (Rank) de uma matriz**: O posto de uma matriz A é o maior número de
  vetores coluna que são linearmente independentes (L.I).
  - Propriedades:
    - `rank(A) = rank(A^T)`
    - ∀ A ∈ ℝ^M×N, `rank(A) ≤ min(m,n)`
    - Se `rank(A) = min(m,n)`, A é *posto completo* ou *full rank*.
  - Em Python, `np.rank(A)`.


- **Matriz inversa**: Dado A ∈ ℝ^N×N, `A*A^-1 = A^-1*A = I`.
  - Se A^-1 existem A é dito nõa singular. Caso contrário, A é dito singular.
  - Para A^-1 existir tem que ser *full rank*.
  - Propriedades:
    - `(A^-1)^-1 = A`
    - `(A*B)^-1 = B^-1*A^01`
    - `(A^-1)^T = (A^T)^-1`
  - Em Python, precisamos do subpacote de álgrebra linear: `np.linalg.inv(A)`.


- **Matrizes ortogonais**: Dados dois vetores x,y, x^Ty = 0, x⊥y,
  A ∈ ℝ^N×N é ortogonal se todas as colunas são ortogonais entre si e
  normalizadas.
  - Se U é ortogonal, `U^T = U^-1`, `U^T*U = I = U*U^-1`.


- **Formas quadráticas**: A ∈ ℝ^N×N e x ∈ ℝ^N,
  `x^TAx = Σ(i=1,N) x_i(Ax)_i = Σ(i=1,N)Σ(j=1,N) A_ij*x_i*x_j`
  - Propriedades:
    - `x^TAx = (x^TAx)^T = x^TA^Tx = x^T(1/2A + 1/2A^T)x`
    - Essa última forma, no meio, é simétrica.


- **Matriz Positivo (Semi-)Definida**: Uma matriz A é simétrica é Positivo
  Definida (PD) se para todos os vetores não-zero x ∈ ℝ^N, x^TAx > 0.
  Denota-se A > 0. A é positivo semi-definida se x^TAx ≥ 0, Denota-se A ≥ 0.
  - Propriedade:
    - Dado A ∈ ℝ^M×N, G = A^TA (matriz de Gram) é sempre positivo
      semi-definida. Se m ≥ N e A for *full rank*, G é positivo definida.


- **Auto-valores e auto-vetores**: Dado A ∈ ℝ^N×N e λ ∈ ℂ, um auto-valor de A, o
  correspondente auto-vetor é dado por `Ax = λx`, x ≠ 0. Isso pde ser calculado
  com `(A - λI)x = 0`.
  - Em Python, `np.linalg.eig(A)` devolve uma lista de auto-valores e um vetor
    de auto-vetores.
  - Propriedades:
    - O traço de A é a soma dos auto-valores: `Tr(A) = Σ(i) λi`
    - O determinante de A é o produto dos auto-valores.


- **Diagonalização**: Uma matriz diagonalizável tem a propriedade que existe
  uma matriz inversível `X` e uma matriz diagonal `Λ` tal que `A = XΛX^-1`
  (chamada de decomposição por diagonalização).
  - Propriedades:
    - Se A for simétrica, todos os auto-valores são reais.
    - Os auto-vetores de A são ortonormais, portanto X é uma matriz ortogonal.
      ```
      A = UΛU^-1 = UΛU^T
      x^TAx = x^TUΛU^Tx = y^TΛy = Σ(i=1,N) λ_i y_i²
      ```

- **_Singular Value Decomposition_ (SVD)**: A ∈ ℝ^N×N, A = UΣV^T, sendo que:
  - U são os auto-vetores de `AA^T`.
  - `Σ = √diag(eig(AA^T))`.
  - V são os auto-vetores de `A^TA`.
  - Propriedades:
    - `A^TA = VΣ^TU^TUΣV^T = VΣ²V^T`
    - `AA^T = UΣV^TVΣ^TU^T = UΣ²U^T`
