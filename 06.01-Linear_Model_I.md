Linear model I
===============

- Pocket PLA

  ```
  POCKET_PLA
    set the pocket weight w^ to w_0 of PLA
    for t = { 0, ..., T-1 } do:
      Run PLA for one update to obtain w_t+1
      Evaluate E_in(w_t+1)
      if w_t_1 is better than w^ in terms of E_in(w_t+1)
        w^ = w_t+1
    return w^
  ```

- In linear regression, we aim to minimize the quadratic error between `h(x)`
  and `y`.

  ```
  E_out(h) = E_out[(h(x)-y)²]
  ```

  Objective: minimize `E_out(h)`.

  If we know `P(x,y)`, the problem is solvable.

  As we don't have it, let's minimize:

  ```
  E_in(h) = 1/N Σ(i=1,N) (h(x_i) - y_i)²
  ```
  With `h` described as:
  ```
  h(x) = Σ(i=1,d) w_i*x)i = w^Tx
  ```

  Defining:
  - `X ∈ ℝ^(N*(d+1))` the matrix of input data with columns `x^T_n`
  - `y ∈ ℝ^N` the vector of labels of the input data.

  Therefore:
  ```
  E_in(w) = 1/N Σ(i=1,N) (w^Tx_i - y_i)² = 1/2 || Xw - y ||²
  ```

  Minimizing `E_in`:
  ```
  E_in(w) = 1/N (w^TX^TXw - 2w^TX^Ty + y^Ty)
  ```

  The linear regressions if made by the followind optimization problem:
  ```
  w_lin =   argmin    E_in(w)
          w ∈ ℝ^(d+1)
  ```

  We want `∇E_in(w) = 0`. Knowing that `[ ∇E_in(w) ]_i = ∂E_in(w)/∂w_i`
  and considerind:
  ```
  ∇w(w^TAw) = (A + A^T)w      #1
  ∇w(w^Tb) = b                #2
  ```

  We can simplify `∇E_in(w)` to:
  ```
   ∇E_in(w) = ∇(1/N (w^TX^TXw - 2w^TX^Ty + y^Ty) )
            = 1/N [ ∇(w^tX^TXw) - ∇(2w^TX^Ty) + ∇(y^Ty) ]
                                                ^^^^^^^
                                                0, because it's a constant

            = 1/N [ ∇(w^tX^TXw) - ∇(2w^TX^Ty) ]
                      ^^^^^^^^      ^^^^^^^^
                      Apply #1      Apply #2

            = 1/N [ (X^TX + X^Tx)w - 2x^Ty ]

            = 1/N [ 2X^Txw - 2X^Ty ]

            = 1/N [ 2(X^Txw - X^Ty) ]

            = 2/N [ X^TXw - X^Ty ]

            = 2x^T/N [ Xw - y ]
  ```

  Calculating the minimum, we get:
  ```
  2x^T/N [ Xw - y ] = 0
  → X^T [ Xw - y ] = 0
  → X^T [ Xw - y ] = 0
  → X^TXw - X^Ty = 0
  → x^TXw = x^Ty
  ```

  If `X^TX` is inversible:
  ```
  w = ((X^TX)^-1 x^T)y
  ```

  The pseudoinverse matrix, `X^† = (X^TX)^-1 x^T` (X-dagger) almost always can
  be calculated.

- Linear Regression Algorithm

  ```
  LINEAR REGRESSION ALGORIGHM
  1. Build the matrix X and the vector y from the labeled data, considering
     x_0 = 1.
  2. Compute the pseudoinverse matrix X^†
  3. Return w_lin = X^†y
  ```

- Linear Regression example
  ```
    X     y     
    1   1
    2   2
    3   1.3
    4   3.75
    5   2.25
  ```

  1. Build the matrix `X` and the vector `y` from the labeled data, considering
     `x_0 = 1`.

     ```
         x_0, always 1
          ^
     X = (1 1)  y = (   1)
         (1 2)      (   2)
         (1 3)      ( 1.3)
         (1 4)      (3.75)
         (1 5)      (2.25)
     ```

  2. Compute the pseudoinverse matrix `X^†`
     ```
     X^TX = (1 1 1 1 1)(1 1) = ( 5 15)
            (1 2 3 4 5)(1 2)   (15 15)
                       (1 3)
                       (1 4)
                       (1 5)

     (X^TX)^-1 = ( 1.1 -0.3)
                 (-0.3  0.1)

     X^† = ( 1.1 -0.3)(1 1 1 1 1) = ( 0.8  0.5 0.2 -0.1 -0.4)
           (-0.3  0.1)(1 2 3 4 5)   (-0.2 -0.1  0   0.1  0.2)
     ```

  3. Return `w_lin = X^†y`
     ```
     w_lin = x^†y = (0.785)
                    (0.425)
     ```

  4. Get y^
     ```
     H = X(X^TX)^-1X^T

     y^ = Hy = (1.21 1.635 2.06 2.495 2.51)
     ```
