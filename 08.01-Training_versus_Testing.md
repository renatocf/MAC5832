Training versus Testing
========================

- Outline
  - From training to testing
  - Illustratve examples
  - Key notion: break point
  - Puzzle

- Example: the final exam
  - Testing:  `IP[|E_in - E_out| > ε] ≤ 2e^-2ε²N`
  - Training: `IP[|E_in - E_out| > ε] ≤ 2Me^-2ε²N`

- Theorem: With probability at least 1-δ:

    E_out(g) ≤ E_in(g) + √(1/2N * ln 2|Η|/δ)

    Dem:
    ```
    Define δ = 2|Η|e^{-2ε²N}
    Isolating ε, ε = √(1/2N * ln 2|Η|/δ)
    
    From Huffing, P[|E_in(g) - E_out(g)| > ε] ≤ 2e^{-2ε²N}
    Substituting δ, P[|E_in(g) - E_out(g)| ≤ ε] > 1-δ

    With probability, at least, 1-δ, ||E_in(g) - E_out(g)| ≤ ε|
    Therefore, E_out(g) ≤ E_in(g) + ε = E_in(g) + √(1/2N * ln 2|Η|/δ)
    ```

  Tradeoff:
    Η small: (because f is simple) → E_out(g) ≈ E_in(g)
    H big: (because f is complex) → E_in(g) ≈ 0

  Usage example:
    M = |Η| = 100
    ε = 0.1
    δ = 0.05

    ε² = 1/2N * ln(2M/δ)
    N  = 1/2ε² * ln(2M/δ)
       = 1/2*(0.01) * ln(200/0.05) ≈ 415

    For a set of 100, an error of 0.1 and a confidence interval of 0.95 (or an
    error of at most 0.05), we need 415 training examples.

- Central questions:
  1. Can we be sure that `E_in ≈ E_out`?
  2. Can we do `E_in ≈ 0`?

- Answers:
  - If |Η| is small, `E_in ≈ E_out`, because `P[Bad] ≤ 2|Η|e^{-2ε²N}`.
    However, `E_in` may not be small...
  - If |Η| is big, `E_out 
